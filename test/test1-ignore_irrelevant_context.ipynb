{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d25c83-7aa9-4da0-abbf-dc814a949cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/notebooks/chatweb3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52beb19-3262-4a57-aff2-5b50c6c4c10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to class registry: chat.simple.SimpleChat\n",
      "Added to class registry: chat.rephrase.RephraseChat\n",
      "Added to class registry: chat.rephrase_cited.RephraseCitedChat\n",
      "Added to class registry: chat.widget_search.WidgetSearchChat\n",
      "Added to class registry: index.weaviate.WeaviateIndex\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Generator\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts.base import BaseOutputParser\n",
    "\n",
    "from chat.base import BaseChat, ChatHistory, Response, streaming_callback_manager\n",
    "from chat.widget_search import ChatOutputParser, WIDGET_INSTRUCTION, SEARCH_INSTRUCTION, TEMPLATE, IDENTIFY_TEMPLATE\n",
    "from chat.base import *\n",
    "from index.weaviate import *\n",
    "from index.widgets import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8f0c68-b9cf-44c7-8ccd-5818b3308d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58767a09-3a4b-47f8-b800-44bf10e8bd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to class registry: __main__.WidgetSearchChat\n"
     ]
    }
   ],
   "source": [
    "@registry.register_class\n",
    "class WidgetSearchChat(BaseChat):\n",
    "    def __init__(self, doc_index: Any, widget_index: Any, top_k: int = 3, show_thinking: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.output_parser = ChatOutputParser()\n",
    "        self.widget_prompt = PromptTemplate(\n",
    "            input_variables=[\"task_info\", \"question\"],\n",
    "            template=TEMPLATE.replace(\"{instruction}\", WIDGET_INSTRUCTION),\n",
    "            output_parser=self.output_parser,\n",
    "        )\n",
    "        self.search_prompt = PromptTemplate(\n",
    "            input_variables=[\"task_info\", \"question\"],\n",
    "            template=TEMPLATE.replace(\"{instruction}\", SEARCH_INSTRUCTION),\n",
    "            output_parser=self.output_parser,\n",
    "        )\n",
    "        self.doc_index = doc_index\n",
    "        self.widget_index = widget_index\n",
    "        self.top_k = top_k\n",
    "        self.show_thinking = show_thinking\n",
    "\n",
    "        self.identify_prompt = PromptTemplate(\n",
    "            input_variables=[\"history\", \"question\"],\n",
    "            template=IDENTIFY_TEMPLATE,\n",
    "            output_parser=self.output_parser,\n",
    "        )\n",
    "\n",
    "    def get_llm(self, new_token_handler):\n",
    "        llm = OpenAI(\n",
    "            temperature=0.0, max_tokens=-1,\n",
    "            # options for streaming\n",
    "            streaming=True, callback_manager=streaming_callback_manager(new_token_handler)\n",
    "        )\n",
    "        return llm\n",
    "\n",
    "    def get_streaming_chain(self, prompt, new_token_handler):\n",
    "        llm = self.get_llm(new_token_handler)\n",
    "        chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "        return chain\n",
    "\n",
    "    def receive_input(self, history: ChatHistory, userinput: str,) -> Generator[Response, None, None]:\n",
    "        userinput = userinput.strip()\n",
    "        # First identify the question\n",
    "        history_string = \"\"\n",
    "        for interaction in history:\n",
    "            history_string += (\"User: \" + interaction.input + \"\\n\")\n",
    "            history_string += (\"Assistant: \" + interaction.response + \"\\n\")\n",
    "        start = time.time()\n",
    "        example = {\n",
    "            \"history\": history_string.strip(),\n",
    "            \"question\": userinput,\n",
    "            \"stop\": \"##\",\n",
    "        }\n",
    "\n",
    "        chat_message_id = None\n",
    "        identify_response = ''\n",
    "        identified_type = None\n",
    "        sent_response = ''\n",
    "\n",
    "        def identify_token_handler(token):\n",
    "            nonlocal chat_message_id, identify_response, identified_type, sent_response\n",
    "            if not self.show_thinking:\n",
    "                return\n",
    "            identify_response += token\n",
    "            if '> ' not in identify_response.strip():\n",
    "                return\n",
    "\n",
    "            if not identified_type:\n",
    "                identified_type, question = identify_response.strip().split(' ', 1)\n",
    "                if identified_type == '<widget>':\n",
    "                    token = \"I think you want a widget for: \" + question\n",
    "                else:\n",
    "                    token = \"I think you're asking: \" + question\n",
    "\n",
    "            sent_response += token\n",
    "            print('token = ', token)\n",
    "            # chat_message_id = send(Response(\n",
    "            #     response=token,\n",
    "            #     still_thinking=False,\n",
    "            #     actor='bot',\n",
    "            #     operation='append' if chat_message_id is not None else 'create',\n",
    "            # ), last_chat_message_id=chat_message_id)\n",
    "\n",
    "        identify_chain = self.get_streaming_chain(self.identify_prompt, identify_token_handler)\n",
    "        identify_response = identify_chain.apply_and_parse([example])[0]\n",
    "        if self.show_thinking:\n",
    "            # send again, but with replace so we save to db\n",
    "            print('sent_response = ', sent_response)\n",
    "            # send(Response(\n",
    "            #     response=sent_response,\n",
    "            #     still_thinking=False,\n",
    "            #     actor='bot',\n",
    "            #     operation='replace',  # this will save to db where append did not\n",
    "            # ), last_chat_message_id=chat_message_id)\n",
    "\n",
    "        duration = time.time() - start\n",
    "        identified_type, question = identify_response.split(' ', 1)\n",
    "        print(f'Intent identification took {duration: .2f}s')\n",
    "        # send(Response(\n",
    "        #     response=f'Intent identification took {duration: .2f}s',\n",
    "        #     actor='system',\n",
    "        #     still_thinking=True,  # turn on thinking again\n",
    "        # ))\n",
    "\n",
    "        chat_message_id = None\n",
    "\n",
    "        def new_token_handler(token):\n",
    "            nonlocal chat_message_id\n",
    "            print('token = ', token)\n",
    "            # chat_message_id = send(Response(\n",
    "            #     response=token,\n",
    "            #     still_thinking=False,\n",
    "            #     actor='bot',\n",
    "            #     operation='append' if chat_message_id is not None else 'create',\n",
    "            # ), last_chat_message_id=chat_message_id)\n",
    "\n",
    "        if identified_type == '<widget>':\n",
    "            widgets = self.widget_index.similarity_search(question, k=self.top_k)\n",
    "            task_info = '\\n'.join([f'Widget: {widget.page_content}' for widget in widgets])\n",
    "            chain = self.get_streaming_chain(self.widget_prompt, new_token_handler)\n",
    "        else:\n",
    "            docs = self.doc_index.similarity_search(question, k=self.top_k)\n",
    "            task_info = '\\n'.join([f'Content: {doc.page_content}\\nSource: {doc.metadata[\"url\"]}' for doc in docs])\n",
    "            chain = self.get_streaming_chain(self.search_prompt, new_token_handler)\n",
    "        start = time.time()\n",
    "        example = {\n",
    "            \"task_info\": task_info,\n",
    "            \"question\": question,\n",
    "            \"stop\": \"User\",\n",
    "        }\n",
    "\n",
    "        result = chain.apply_and_parse([example])[0]\n",
    "        duration = time.time() - start\n",
    "        history.add_interaction(userinput, result)\n",
    "        print('result = ', result)\n",
    "        print(f'Response generation took {duration: .2f}s')\n",
    "        # send(Response(result, operation='replace'), last_chat_message_id=chat_message_id)\n",
    "        # send(Response(response=f'Response generation took {duration: .2f}s', actor='system'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39d1bc4-83df-49e4-a688-19bf9437acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_index = WeaviateIndex('IndexV1', 'content') \n",
    "widget_index = WeaviateIndex('WidgetV1', 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c263b6c8-6f76-4dbb-aaee-eead1eb7d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions, session_id = [], str(uuid.uuid4())\n",
    "history = ChatHistory(interactions, session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8dd8e13-8038-469b-a02d-1b36bbc3e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = WidgetSearchChat(doc_index, widget_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a1966cd-9207-410f-8504-85eac04455e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Given the following conversation and a follow up response input, determine if the input is related to a command to invoke using a widget or if it is a search query for a knowledge base. If it is a widget, return the appropriate keywords to search for the widget, as well as all relevant details to invoke it. If it is a search query, rephrase as a standalone question. You should assume that the query is related to web3.\n",
      "\n",
      "## Example:\n",
      "\n",
      "Chat History:\n",
      "User: I'd like to make transfer ETH\n",
      "Assistant: Ok I can help you with that. How much and to which address?\n",
      "\n",
      "Input: 2 ETH to andy\n",
      "Ouput: <widget> transfer of 2 ETH currency to andy\n",
      "\n",
      "## Example:\n",
      "\n",
      "Chat History:\n",
      "User: Who created Ethereum?\n",
      "Assistant: Vitalik Buterin\n",
      "User: What about AAVE?\n",
      "Assistant: Stani Kulechov\n",
      "\n",
      "Input: When was that?\n",
      "Output: <query> When were Ethereum and AAVE created?\n",
      "\n",
      "## Example:\n",
      "\n",
      "Chat History:\n",
      "User: Who created Ethereum?\n",
      "Assistant: Vitalik Buterin\n",
      "\n",
      "Input: What is AAVE?\n",
      "Output: <query> What is AAVE?\n",
      "\n",
      "## Example:\n",
      "\n",
      "Chat History:\n",
      "User: What's my balance of USDC?\n",
      "Assistant: Your USDC balance is <|balance('USDC')|>\n",
      "\n",
      "Input: cost of ETH\n",
      "Output: <widget> price of ETH coin given USDC balance\n",
      "\n",
      "## Example:\n",
      "\n",
      "Chat History:\n",
      "\n",
      "\n",
      "Input: what are you capable of?\n",
      "Output:\u001b[0m\n",
      "token =  I think you're asking: What\n",
      "token =   are\n",
      "token =   the\n",
      "token =   capabilities\n",
      "token =   of\n",
      "token =   the\n",
      "token =   assistant\n",
      "token =  ?\n",
      "token =  \n",
      "sent_response =  I think you're asking: What are the capabilities of the assistant?\n",
      "Intent identification took  2.20s\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat are you capable of?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mWidgetSearchChat.receive_input\u001b[0;34m(self, history, userinput)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_index\u001b[38;5;241m.\u001b[39msimilarity_search(question, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m--> 122\u001b[0m     task_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSource: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs])\n\u001b[1;32m    123\u001b[0m     chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_streaming_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_prompt, new_token_handler)\n\u001b[1;32m    124\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_index\u001b[38;5;241m.\u001b[39msimilarity_search(question, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m--> 122\u001b[0m     task_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSource: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs])\n\u001b[1;32m    123\u001b[0m     chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_streaming_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_prompt, new_token_handler)\n\u001b[1;32m    124\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'url'"
     ]
    }
   ],
   "source": [
    "chat.receive_input(history, 'what are you capable of?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e36a359-fbb7-4ab9-8561-5a39bc220249",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = doc_index.similarity_search('what are your capabilities', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84112e8-25b2-4bf9-9183-6039cb124738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"molded by their previous experiences.\\n\\n5.3. Do-ers vs Talkers\\n\\nHire do-ers who can express themselves, not talkers who can’t do. Doers who can’t articulate are tricky. They may be ok in some narrow tech situations, but we can’t have too many of them on the team either.\\n\\n5.4. Hire with a Goal\\n\\nEach new person must have clear responsibilities, ideally with aggressive numerical targets with about a 70% chance of success.\\n\\n5.5. No Titles\\n\\nDon’t hire people who are worried about titles. It’s not a deal breaker, but definitely not a good sign.\\n\\n5.6. Mission over Money\\n\\nDon’t hire people who are overly fixated on salaries and comp. We should pay people fairly.\\n\\n5.7. When in doubt, don’t hire.\\n\\nIf you have doubts during the hiring process, don’t hire. Small doubts in the interview stage always turn into a big problem in the future.\\n\\n6. Leadership Style\\n6.1. Don’t try to motivate people who are not self motivated.\\xa0\\n\\nIt’s like dragging a dead horse. It’s impossible. It’s not worth it. It’s also impossible to motivate people who don’t share your mission or values, or don’t like you as a leader, or are just lazy. Let them work somewhere else. People are either motivated, or they are not. Only work with the self motivated ones.\\n\\nWe work remotely. It’s easy for lazy people to slack off given no one is watching them. But this is a blessing in disguise. People can slack off for a day, a week, or maybe even a month. But after a couple of months, they won’t have the results to show for it, and you will know. Remote work actually makes it easy for you to identify them over time. Let go of the unmotivated people on your team, as soon as you identify them.\\n\\n6.2. Never micro-manage\\n\\nIt takes more time to micromanage than doing the work yourself. If you need to micromanage, you should let that person go.\\n\\n6.3. Qualification for Interviews, Results Afterwards\\n\\nUse “years of experience” only during the recruiting process. After a person is on the team, use results to measure performance.\\n\\n6.4. Do: Work hard, stick to our values, and lead by example.\\n7. Targets, OKR/KPIs\\n\\nUse output metrics (users, revenue, market share), not input metrics (tasks, features, meetings, hours worked).\\n\\n7.1 Don’t take targets too seriously\\n\\nThere are many potential downsides with targets, or goal setting. Much has been written about this. I won’t go into the details. They include: a sense of failure when you can’t achieve them, not working hard after an easy goal is achieved, inflexible direction, etc.\\n\\nMy biggest issues with targets are: 1. they are never accurate, or scientific. It’s always some random guess. In our industry, market conditions change too quickly. 2. They take too much time (costly) to discuss.\\n\\nFor these reasons, set a goal, work towards it, set a new goal if you achieved it already. Don’t take it too seriously. Don’t get too fixated on it.\\n\\nOne example to close this topic off. When Binance started, we set a goal to become the world’s top 10 exchanges in 3 years. We became the world's NO. 1 exchange in 5 months. We didn’t stop.\\n\\n8. Business Deals\\n8.1. Keep deals simple\\n\\nComplex deals with many variables often fail, even after they are signed. Complex deals are hard to understand. There is often confusion or misinterpretation. One party always feels screwed in some way and wants to change something. Keep the deals simple: party A provides this and gets that; party B provides this and gets that.\\n\\n8.2. Say No Early\\xa0\\n\\nToo many people waste too much time on useless “partnership” discussions. When your mind space is spent on these useless discussions, you won’t be thinking about useful partnerships.\\n\\n8.3. No exclusivity\\n\\nLong-term mutual win-win relationships don’t need exclusivity. People asking for exclusivity are typically insecure about competitiveness or the value they\", lookup_str='', metadata={}, lookup_index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
